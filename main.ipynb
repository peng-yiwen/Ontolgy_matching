{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXTO Ontology Matching\n",
    "\n",
    "* To install dependencies, please run\n",
    "`\n",
    "pip install -r requirements.txt\n",
    "`.\n",
    "* Please change the data_path to corresponding dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/schema-wikidata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse as onto_parser\n",
    "from embedding import Embedding, similarity_evaluate, pool_simi\n",
    "from string_match import string_matching\n",
    "from evaluate import Precision_Recall_F1\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse input ontologies\n",
    "onto_parser.rdf2csv(data_path+\"source.rdf\", \"source.csv\")\n",
    "onto_parser.rdf2csv(data_path+\"target.rdf\", \"target.csv\")\n",
    "onto_parser.reference2csv(data_path+'reference.rdf', 'reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and pre-trained model\n",
    "df_reference, df_source, df_target = pd.read_csv('reference.csv'), pd.read_csv('source.csv'), pd.read_csv('target.csv')\n",
    "gold_standard = df_reference.apply(lambda row: (row['Class1_id'], row['Class2_id']), axis=1).tolist()\n",
    "source_classes_kl = df_source.to_dict(orient='list')\n",
    "target_classes_kl = df_target.to_dict(orient='list')\n",
    "\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "alpha = 0.5\n",
    "cf_limit = 0.4\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* String Matching and Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "string matching: 100%|██████████| 343/343 [00:17<00:00, 19.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating cosine similarity by using labels knowledge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate embedding for source ontology classes: 100%|██████████| 343/343 [00:00<00:00, 9628.08it/s]\n",
      "generate embedding for target ontology classes: 100%|██████████| 343/343 [00:00<00:00, 10088.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating cosine similarity by using descriptions knowledge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate embedding for source ontology classes: 100%|██████████| 343/343 [00:06<00:00, 49.14it/s]\n",
      "generate embedding for target ontology classes: 100%|██████████| 343/343 [00:07<00:00, 43.04it/s]\n"
     ]
    }
   ],
   "source": [
    "embedder_desc = Embedding(sbert_model, name='SBert')\n",
    "embedder_label = Embedding(w2v_model, name='Glove')\n",
    "\n",
    "simi_matrix = string_matching(source_classes_kl['label'], target_classes_kl['label'])\n",
    "print('Now calculating cosine similarity by using labels knowledge')\n",
    "label_simi = similarity_evaluate(source_classes_kl['label'], target_classes_kl['label'], embedder_label, simi_matrix, metrics='cos')\n",
    "print('Now calculating cosine similarity by using descriptions knowledge')\n",
    "desc_simi = similarity_evaluate(source_classes_kl['comment'], target_classes_kl['comment'], embedder_desc, simi_matrix, metrics='cos')\n",
    "\n",
    "# weighted average\n",
    "class_simi = pool_simi(desc_simi, label_simi, pooling='weighted', weights=[alpha,1-alpha]) + simi_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Candidate Selection and Max-Weight Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "\n",
    "def candidate_slection(similarity, k=5):\n",
    "    row_max_indices = np.argpartition(similarity, -k, axis=1)[:, -k:]\n",
    "    col_max_indices = np.argpartition(similarity, -k, axis=1)[:, -k:]\n",
    "    result = np.zeros_like(similarity)\n",
    "    result[np.arange(similarity.shape[0])[:, np.newaxis], row_max_indices] = similarity[np.arange(similarity.shape[0])[:, np.newaxis], row_max_indices]\n",
    "    result[np.arange(similarity.shape[0])[:, np.newaxis], col_max_indices] = similarity[np.arange(similarity.shape[0])[:, np.newaxis], col_max_indices]\n",
    "    return result\n",
    "\n",
    "new_simi = candidate_slection(class_simi, k)\n",
    "graph = csr_matrix(new_simi + 1e-9)\n",
    "row_ixs, col_ixs = min_weight_full_bipartite_matching(graph, maximize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Final alignment generation\n",
    "* Note: using the commented final_alignments when evaluating on nell-dbpedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_Recall_F1 score: [0.994 0.977 0.985]\n"
     ]
    }
   ],
   "source": [
    "final_alignments = []\n",
    "for i in range(len(row_ixs)):\n",
    "    if class_simi[row_ixs[i], col_ixs[i]] > cf_limit:\n",
    "        final_alignments.append((source_classes_kl['id'][row_ixs[i]], target_classes_kl['id'][col_ixs[i]]))\n",
    "        # final_alignments.append((target_classes_kl['id'][col_ixs[i]], source_classes_kl['id'][row_ixs[i]]))\n",
    "# evaluate\n",
    "print('Precision_Recall_F1 score:', Precision_Recall_F1(gold_standard, final_alignments).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
